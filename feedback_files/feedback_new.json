{
    "DeskReviewer": {
        "Accept": true,
        "Feedback": "Accept.\n\nThe paper appears to be relevant to a conference on topics related to ACM Student Research Competition, Industry Track, and software maintenance, specifically focusing on language models and their applications in code documentation. The introduction mentions the importance of improving documentation practice in industry and the need for better-coupled code-documentation updates, which aligns with the themes of the conference.\n\nThe paper's goal to propose a new dataset, CoDocBench, aimed at training and evaluating language models in tasks related to helping developers better couple code and document changes is also relevant to the conference. The focus on improving documentation practice and reducing the poorly-coupled Code-Documentation update rate of 33% reported by Schrek et al. [7] aligns with the conference's emphasis on software engineering education, training, and industry-track topics.\n\nOverall, the paper seems to be a good fit for the conference, and its focus on language models and code documentation makes it relevant to the Industry Track and other relevant topics."
    },
    "Questions": {
        "I.": "Here are some open-ended questions about the paper:\n\n* What specific aspect of software maintenance does the proposed dataset, CoDocBench, focus on?\n* How does the problem of poorly-coupled code-documentation update rate of 33% reported by Schrek et al. [7] relate to the goals of this work?\n* Why is documentation practice in industry considered to need improvement?\n* What are the two initial tasks that CoDocBench aims to automate, and what benefits do they offer for improving code maintenance?\n* How does the dataset CoDocBench differ from other datasets used in LLM applications to software engineering tasks?\n* What are some potential challenges or limitations of using machine-learning models like LLMs to address this specific problem in code documentation?",
        "II.": "Here are some open-ended questions about the paper:\n\n* What motivated you to select a specific subset of Python projects from GitHub, and how did you determine their diversity and representativeness?\n* Can you explain why you chose to use regular expressions for pairwise commit change detection per function, and what benefits did this approach provide in terms of data extraction?\n* How did you validate the accuracy of your dataset using Tree-Sitter and the function_parser package, and what types of errors or inconsistencies were most frequently encountered during validation?\n* What insights do the scatterplots (Fig. 3a and b) reveal about the relationship between code length, docstring length, and their respective diff statistics?\n* Can you discuss the implications of the skewed distribution of project sizes in your dataset (e.g., why do the top 25 projects account for around 60% of the total samples)?\n* How does the diversity of the collected codebases across different projects contribute to the overall value and usefulness of the CoDocBench dataset?",
        "III.": "Here are some open-ended questions about the paper:\n\n* What are the main challenges in the task of aligning code and docstrings for old and new versions, according to the researchers?\n* How do the researchers define a \"correct alignment\" for the two research questions (RQ1 and RQ2)?\n* Can you explain why the researchers chose to use edit distance as a metric for measuring alignment between generated code/docstrings and their references?\n* What are some potential limitations of using 0-shot learning, and how does this approach compare to using contextual information or retrieval-based prompting strategies?\n* How do the results in Table II suggest that the models struggle with aligning code and docstrings, and what implications do these findings have for future work in this area?\n* Can you summarize the main differences between the two models (Mixtral-8 \u00d722B and Meta Llama-3.1 405B) used by the researchers, and how do these differences impact their performance on RQ1 and RQ2?",
        "V.": "Can you elaborate on how the limitation affects the tracking mechanism? \n\nHow does the focus on the main branch impact the dataset's ability to track changes over time? \n\nWhat are some potential scenarios where file restructuring or renaming might be common, and how might this affect the dataset's utility? \n\nDo you think the trade-off between consistency and stability in the tracked code and docstring is worth it, and why or why not?",
        "VI.": "What kind of data or results does the publicly available source code contain, and what is its purpose in relation to the research presented in this paper?",
        "ACKNOWLEDGMENTS": "What is the purpose of providing this acknowledgment in the context of the research paper?",
        "REFERENCES": "Can you summarize the main topics of these references in a few sentences? \n\nAre they related to software development, testing, or knowledge engineering?\n\nDo any of these references suggest that natural language processing (NLP) is being used to analyze or improve code and documentation?\n\nAre there any notable authors among the listed papers, and if so, what are their areas of expertise?"
    },
    "Grammar": {
        "I.": {
            "Accept": true,
            "Feedback": "Accept (Introduction)\nReject \n(I INTRODUCTION)\n-> should be (I) Introduction\n\n(60-80% of overall software budgets [1], and thus constitute an attractive target for efforts to manage costs.)\n-> should have a comma after \"budgets\" or be rephrased.\n\n(an attractive target for efforts to manage costs.)\n-> should be (an attractive target for efforts to manage costs)\n\n(As language models applications to software engineering tasks continue to burgeon, the value of LLM for software maintenance has been recognized.)\n-> should have a comma after \"burgeon\".\n\n(the value of LLM for software maintenance has)\n-> should be (the value that LLM can bring to software maintenance)\n\n(Various applications of LLM to maintenance have been promoted, such as automated code repair [2], automated response to code review comments [3], industrial-scale generalized code maintenance support [4], and even automatic response to submitted GitHub issues [5], with proposed changes.)\n-> should be (various applications of LLM in software maintenance have been explored)\n\n(Based as they are on machine-learning, all the above depend on realistic, well-curated datasets.)\n-> should be (all the above rely on realistic, well-curated datasets based on machine learning)\n\n(Our goal in this work is to propose a new dataset, for a specific aspect of software maintenance . . .supporting well-documented code changes.)\n-> should have an article (\"a\") before \"this\".\n\n(It has been reported that documentation practice in industry really needs improvement [6], and that many maintenance difficulties arise as a result.)\n-> should be (documentation practices in the industry are reported to need improvement)\n\n(Our work is focused on the specific problem in code documentation raised in Schrek et al . [7], viz., how code and the associated natural language description (DocStrings) are not always kept up to date;)\n-> should have a comma after \"description\".\n\n(It has been reported that Docstrings are updated only around 33% of the time that code is changed; this phenomenon suggests that LLMs could help programmers better document changes.)\n-> should be (this phenomenon highlights the need for improved docstring updates)\n\n(We introduce a new dataset, CoDocBench , aimed at training and evaluating language models in tasks related to helping developers better couple code and document changes.)\n-> no grammar error\n\n(Figure 1 presents an example where an argument of the)\n-> should be (an example illustrating) \n\n(Fig. 1: An example where docstring and function were changed simultaneously in the same commit)\n-> should have a comma after \"commit\".\n\n(function is dropped and docstring-function pair were updated to accommodate the change.)\n-> should be (the corresponding code changes were made to update this pair)\n\n(Specifically, we envision two initial tasks based on this dataset.)\n-> no grammar error\n\n(First, given old code, old associated docstring, and new version of the code: create a new docstring better aligned with the new code; second,)\n-> should be (the proposed tasks for this dataset include)\n\n(given old code, old associated docstring, and a new version of the code: create a new docstring better aligned with the new code;)\n-> has extra information\n\n(second, given old docstring, old associated code, and a new docstring reflecting the intended new function of the code), generate new implementation aligned with the new docstring.)\n-> no grammar error\n\n(If such tasks could be automated, one can expect that the poorly-coupled Code-Documentation update rate of 33% reported by Schrek et al . [7] could be improved, thus leading to better documented and more maintainable code.)\n-> should be (the success of these tasks would likely lead)\n\n(CoDocBench consists of 4573 high-quality samples of coupled code-documentation changes from GitHub;)\n-> no grammar error\n\n(we have selected changes where developers have indeed changed both the code and the documentation in the same commit .)\n-> has extra information\n\n(In the following we describe: 1) the collection & curation procedure)\n-> should be (In the following, we outline)\n\n(2) the dataset per se , and 3) and some illustrative studies of the data.)\n-> should have a comma after \"se\"\n\n(Some grammar errors in other sentences were not reported"
        },
        "II.": {
            "Accept": true,
            "Feedback": "Accept for Figure captions (e.g., \"Fig. 3:\", \"Fig. 4:\") and table titles (e.g., \"TABLE I:\") as they are commonly referred to in academic writing with initial capitalization.\n\nReject for all other grammar-related issues."
        },
        "III.": {
            "Accept": true,
            "Feedback": "I can check the grammar of this text for you.\n\nHere's my review:\n\nAccept (for most of the text)\nReject \n\n(there are several instances where the grammar is incorrect or unclear)"
        },
        "V.": {
            "Accept": true,
            "Feedback": "Accept \n\nNote: The paper appears to be well-structured, and grammar is correct throughout. However, it would benefit from a thorough proofread for clarity, consistency, and sentence-level editing."
        },
        "VI.": {
            "Accept": true,
            "Feedback": "Accept"
        },
        "ACKNOWLEDGMENTS": {
            "Accept": false,
            "Feedback": "Reject"
        },
        "REFERENCES": {
            "Accept": true,
            "Feedback": "Accept \nAccept \nAccept \nReject \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept \nAccept"
        }
    },
    "Novelty": {
        "I.": {
            "Feedback": "Accept.\n\nThis paper proposes a new dataset, CoDocBench, aimed at training and evaluating language models in tasks related to helping developers better couple code and documentation changes. The authors aim to improve the poorly-coupled Code-Documentation update rate of 33% reported by Schrek et al., which suggests that the proposed dataset and methods have the potential to make a significant impact on software maintenance.\n\nThe paper provides a clear overview of the problem, the dataset, and the methodology for gathering and evaluating it. The authors also present their findings from experiments using existing models, including Llama 3.1 405B Mixtral 8 times 22B, and propose a multi-stage fine-tuning strategy to improve performance.\n\nThe paper's novelty lies in its focus on code documentation alignment, which is an important aspect of software maintenance. The proposed dataset and methods have the potential to benefit software maintenance and programming education, and the authors' experiments demonstrate that their approach can lead to better performance compared to existing models.\n\nOverall, the paper presents a well-structured and well-researched proposal for a new dataset and methodology in the field of natural language processing for software engineering tasks.",
            "Accept": true
        },
        "II.": {
            "Feedback": "Accept. \n\nThe paper presents a novel dataset called CoDocBench, which provides a large-scale collection of coupled changes to code and documentation mined from actual high-quality GitHub projects. The dataset is designed to support research on code explanation generation tasks, including the proposed task of generating detailed code explanations for software maintenance and programming education.\n\nThe authors also propose a multi-stage fine-tuning strategy for this task and evaluate their refined training dataset against larger unrefined data. They demonstrate that their refined dataset allows models to achieve better performance in explanation generation tasks compared to larger unrefined data.\n\nFurthermore, the paper presents a controlled experiment on code documentation generation for two Python functions, which reveals that professionals and students were unaware of or unable to apply prompt engineering techniques. The authors also propose a framework for validating requirements changes and estimating rework effort in software maintenance.\n\nThe novelty of this paper lies in its comprehensive dataset collection methodology and its contributions to research on code explanation generation tasks, prompt engineering, and software maintenance estimation.",
            "Accept": true
        },
        "III.": {
            "Feedback": "I would Reject this paper as novel.\n\nThe reasons for my decision are:\n\n1. The paper discusses several existing research papers and datasets, including CoDocBench, CodeExp, DocuMint, and Can Developers Prompt. While it provides some new insights and results from these studies, the overall scope of the paper is not particularly original or groundbreaking.\n2. The authors' proposed framework for understanding rework effort in software maintenance is based on existing research, including the work of Suber Cui et al. (2019) and others. The proposed framework is an extension of this existing research, rather than a fundamentally new idea.\n3. The paper does not introduce any significantly new or innovative approaches to code documentation alignment or rework effort estimation. Instead, it builds upon existing work in these areas, providing some additional insights and results from experiments with large language models.\n4. The paper's focus on the challenges of generating high-quality docstrings for code changes is an important one, but it has been explored by other researchers before (e.g., Cui et al., 2019; CodeExp). While the authors' results are interesting, they do not represent a fundamentally new direction in research.\n\nOverall, while the paper provides some valuable insights and contributes to ongoing discussions in the field of software maintenance, its scope is not particularly novel or groundbreaking.",
            "Accept": false
        },
        "V.": {
            "Feedback": "I would Reject this paper.\n\nThe reasons for rejecting this paper are:\n\n1. Lack of novelty: The paper discusses existing research on code documentation alignment, change-related tasks, and software maintenance, which suggests that it is not a novel contribution to the field.\n2. Limited originality: The paper's proposed framework and methodology, while described in detail, do not appear to introduce any significantly new or innovative ideas.\n3. Repetition of existing results: The paper reports on experiments with existing models (Llama 3, Mixtral) and datasets (CodeExp), which suggests that the results are not novel or surprising.\n4. Lack of a clear contribution: The paper does not clearly articulate what new insights or contributions it makes to the field of software maintenance and code documentation alignment.\n\nOverall, while the paper may provide some useful information on existing research in the field, it does not appear to offer any significant new insights or contributions that would warrant its consideration as a novel paper.",
            "Accept": false
        },
        "VI.": {
            "Feedback": "Accept.\n\nI chose to accept this paper as novel because it presents several original contributions to the field of software maintenance and code documentation generation. The paper proposes a new dataset, called CodeExp, which is specifically designed for training models that can generate high-quality docstrings. The authors also present a multi-stage fine-tuning strategy and baseline models for the task, which shows that their approach outperforms existing methods.\n\nFurthermore, the paper explores the importance of prompt engineering techniques in generating concise and useful documentation, which is an under-researched area. The controlled experiment with professionals and computer science students reveals that many developers are unaware of or unable to apply prompt engineering techniques, making this research a valuable contribution to the field.\n\nAdditionally, the paper addresses a significant problem in software maintenance, namely estimating rework effort, by proposing a new framework for evaluating requirements changes. This is a novel approach that has not been explored before, and its validation through case studies makes it a credible contribution to the field.\n\nOverall, while some of the research may be related to existing work, the paper presents several original contributions and explores important open problems in software maintenance, making it a valuable addition to the literature.",
            "Accept": true
        },
        "ACKNOWLEDGMENTS": {
            "Feedback": "I would Accept this paper.\n\nThe paper proposes several novel contributions to the field of software maintenance and programming education. The authors present a new natural large dataset of coupled changes to code and documentation, which is a significant improvement over existing datasets. They also propose a multi-stage fine-tuning strategy for training models on this dataset, which achieves better performance in explanation generation tasks compared to larger unrefined data.\n\nThe paper also investigates the efficacy of small language models (SLMs) for generating high-quality docstrings and introduces DocuMint as a large-scale supervised fine-tuning dataset. The authors benchmark the performance of Llama 3.8B and CodeGemma 7B on various metrics, including conciseness and clarity scores.\n\nFurthermore, the paper reports on a controlled experiment with professionals and computer science students to investigate the effectiveness of prompting SLMs for code documentation generation. The results show that fine-tuned models can generate well-structured long docstrings comparable to human-written ones.\n\nThe authors also propose a framework for estimating rework effort during software maintenance, which is validated through a case study. This framework addresses a significant problem in software maintenance and provides a new approach to managing rework effort.\n\nOverall, the paper makes several novel contributions to the field of software maintenance and programming education, and its results have the potential to impact the development of more effective code documentation tools and methods for estimating rework effort.",
            "Accept": true
        },
        "REFERENCES": {
            "Feedback": "I would Reject this paper.\n\nAlthough the paper discusses various topics related to software maintenance, code documentation, and machine learning, it lacks novelty. The ideas presented in the paper are not entirely new or groundbreaking. \n\nFor example, the authors mention that existing code-to-text generation models often produce only high-level summaries of code, which is a common limitation in this area of research. However, they do not present any novel solutions to address this limitation.\n\nSimilarly, the authors discuss the importance of prompt engineering for effective code documentation generation, but they do not provide any new insights or methods for improving prompt engineering techniques.\n\nFurthermore, the paper's discussion of software maintenance schedules and rework effort estimation is not particularly original. The authors mention that inconsistent or incorrect effort estimation can increase the risk of problematic project completion times, but they do not present any novel solutions to address this issue.\n\nThe only potentially novel aspect of the paper is the proposed framework for validating requirements changes and estimating rework effort. However, without more information about the methodology and results of this framework, it is difficult to determine whether the authors have made a significant contribution to this area of research.\n\nOverall, while the paper may provide some useful insights and ideas, its lack of novelty makes it difficult to consider it novel.",
            "Accept": false
        }
    },
    "FactChecker": {
        "I.": {
            "Feedback": "[Reject]\n\nSpecific corrections:\n\n1. The introduction claims that software maintenance activities consume 60-80% of overall software budgets, but there is no reliable source cited to support this claim.\n2. The text states that documentation practice in industry needs improvement and cites Schrek et al., who found that Docstrings are updated only around 33% of the time code is changed. However, there is no concrete source provided for these claims.\n3. Figure 1 is mentioned as an example but its relevance to the main text is unclear.\n4. The initial tasks based on the CoDocBench dataset are not clearly specified in the text.\n\nAdditional thoughts:\n\n* It appears that the authors have proposed a new dataset, CoDocBench, aimed at training and evaluating language models for code documentation maintenance tasks. However, without concrete sources to support their claims about software maintenance budgets and documentation practice, the credibility of these statements is questionable.\n* The introduction's mention of Schrek et al.'s findings on Docstring update rates seems to be a basis for the authors' proposed solution. However, it would be beneficial to include the actual source citation (e.g., [7]) in the text to provide concrete evidence.\n* The two initial tasks mentioned as being based on CoDocBench seem plausible, but more context is needed to fully understand their significance and relevance to code documentation maintenance.\n\nTo improve the accuracy of this text, the authors should provide reliable sources for the claims made about software maintenance budgets, documentation practice, and Schrek et al.'s findings.",
            "Accept": false
        },
        "II.": {
            "Feedback": "[Reject]\n\nThe text appears to be a technical report or manuscript, but it lacks clarity and formatting consistency. Here are some specific corrections:\n\n1. The report seems to use inconsistent formatting for section headings, bullet points, and paragraphs.\n2. Some sentences are incomplete or lack context, making it difficult to understand the author's intent.\n3. There is no clear introduction or background information on the CoDocBench dataset or its methodology.\n4. The text assumes a high level of technical knowledge in Git, Python, and software engineering, which may be unfamiliar to non-experts.\n\nSpecifically, I would like to see more context on:\n\n* What problem does CoDocBench aim to solve?\n* Why is it important for maintaining code consistency and documentation?\n\nMy own take on the facts is that CoDocBench appears to be a dataset collection methodology designed to gather information about code changes and their corresponding docstring modifications. The use of regular expressions, PyDriller, and Tree-Sitter suggests a focus on automated analysis and parsing of Git data.\n\nHowever, without more context or information on the problem being addressed, it is difficult to fully understand the significance or relevance of CoDocBench.\n\nCorrections:\n\n* Add an introduction or background section that explains the purpose and importance of CoDocBench.\n* Use consistent formatting throughout the report.\n* Provide additional explanations or examples to clarify technical concepts and assumptions.\n* Consider adding a clear explanation of the dataset's schema and structure (Table I).\n* Include more context on how the dataset will be used, such as in machine learning or natural language processing applications.\n\nOverall, while the text appears to be well-researched and technically accurate, it could benefit from improved clarity, organization, and contextualization.",
            "Accept": false
        },
        "III.": {
            "Feedback": "[Reject]\n\nThe provided text contains several inaccuracies:\n\n1. The statement about Mixtral-8 \u00d722B having 47 billion parameters while activating only 13 billion for each token is false, according to the paper itself.\n\n2. The claim that Meta Llama-3 models have improved upon previous generations primarily through improved data quality, greater diversity, and expanded training scale is not supported by the provided references.\n\n3. Some of the numbers and results presented in tables II and III are unclear or inconsistent, which may indicate errors or inaccuracies.\n\n4. Statements about RQ1 and RQ2 definitions and rationales are ambiguous, lacking clarity on the specific metrics used for correct alignment.\n\nTo verify these facts, I recommend consulting Wikipedia's page on \"Large language models\" as well as other reliable sources to confirm the accuracy of the statements made in the provided text.",
            "Accept": false
        },
        "V.": {
            "Feedback": "[Reject]\n\nSpecific corrections:\n\n1. The statement implies that tracking changes to a function by its file name is sufficient, which is not necessarily true. A more accurate approach might involve tracking changes based on the function's implementation or its dependencies.\n2. There is no clear information on why the dataset only tracks changes within the main branch and ignores feature branches or pull requests. The text mentions that these branches may not always reflect high-quality code, but it does not provide a clear explanation for this limitation.\n\nAdditional analysis: While tracking changes to a function based on its file name can be convenient, it has limitations as pointed out in the statement. A more robust approach would involve combining multiple methods of change tracking, such as file-based tracking and implementation-based tracking, to ensure consistency and accuracy.\n\nNote: Wikipedia does not specifically mention this limitation, but similar concepts are discussed in other articles, such as the \"git\" documentation and the \"version control\" section of the Wikipedia article on \"Software development\".",
            "Accept": false
        },
        "VI.": {
            "Feedback": "[Reject]\n\nThe claim that the source code is publicly available on both Zenodo and GitHub, but fails to provide a working link or verify if the provided DOI-identifier is accurate. Additionally, it's worth noting that while Zenodo does store and manage open research data, including source code, it's not necessarily the primary repository for software development projects.",
            "Accept": false
        },
        "ACKNOWLEDGMENTS": {
            "Feedback": "**Closed-ended question**\nA closed-ended question is any question for which a researcher provides research participants with options from which to choose a response.[1] Closed-ended questions are sometimes phrased as a statement that requires a response A closed-ended question contrasts with an open-ended question, which cannot easily be answered with specific information Examples of closed-ended questions that may elicit a \"yes\" or \"no\" response include: Similarly, variants of the above closed-ended questions that possess specific responses are: At the same time, there are closed-ended questions that are sometimes impossible to answer correctly with a yes or no without confusion, for example: \"Have you stopped taking heroin?\" (if you never took it) or \"Who told you to take heroin?\"; see \"loaded question\" and \"\"leading question\" A study by the University of Cincinnati found 20 to 40 percent of Americans will provide an opinion when they do not have one because of social pressure, using context clues to select an answer they believe will please the questioner A classic example of this phenomenon was the 1947 study of the fictional Metallic Metals Act.[2] There are alternative names for the same concept...\n[Read more](https://en.wikipedia.org/wiki/Closed-ended_question)",
            "Accept": false
        },
        "REFERENCES": {
            "Feedback": "[Reject]\n\nWhile the text mentions \"best practice\" and references Wikipedia, there are inaccuracies in the provided information.\n\nSpecific corrections:\n\n1. The definition of best practice is oversimplified. While it is generally accepted as a superior method or technique, its application can be complex and nuanced. Best practices often involve a combination of evidence-based methods, industry standards, and organizational context.\n2. The text implies that best practices are used to achieve quality as an alternative to mandatory standards. However, this is not always the case. Mandatory standards, such as ISO 9000 and ISO 14001, can provide a framework for organizations to follow, while best practices may be used to supplement or refine these standards.\n3. The text suggests that some consulting firms specialize in offering ready-made templates to standardize business process documentation. While this is true, it is not accurate to imply that all best practices involve standardized templates.\n\nA more accurate understanding of best practice would acknowledge its complexity and nuance, as well as the need for context-dependent application.",
            "Accept": true
        }
    }
}