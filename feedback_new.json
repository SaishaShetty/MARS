{
    "DeskReviewer": {
        "Accept": false,
        "Feedback": "I reject this paper as not relevant to ASPLOS 2026 or WACI 2025.\n\nThe topics mentioned in the paper, such as automated code repair, response to code review comments, and industrial-scale generalized code maintenance support, are more relevant to ASPLOS than WACI. However, the focus of the paper is on improving documentation practice in industry and creating a new dataset for training and evaluating language models in tasks related to helping developers better couple code and documentation changes.\n\nWhile the topic of software maintenance and its intersection with natural language processing is certainly relevant to both conferences, the specific focus of this paper seems more aligned with WACI's goals than ASPLOS. Therefore, I reject the paper as not being a good fit for ASPLOS 2026 or WACI 2025."
    },
    "Questions": {
        "I.": "Can you elaborate on what motivated the authors to focus on this specific aspect of software maintenance, namely supporting well-documented code changes? What were they trying to address with CoDocBench?",
        "II.": "Can you explain the criteria used to select the top 200 Python projects on GitHub for inclusion in the CoDocBench dataset? \n\nHow does the use of regular expressions enable the efficient detection of function definitions and text enclosed within triple quotes inside functions, allowing the separation of docstrings from actual code?\n\nWhat is the purpose of applying a 50-50 train-test split to the curated list of projects, and how does this ensure an even distribution of data for training and evaluation? \n\nHow does the dataset's project distribution skew towards certain types of projects, with the top 25 projects accounting for around 60% of the total samples?\n\nAre there any specific challenges in maintaining consistency in descriptive text (e.g., docstrings) as the underlying code evolves, according to Figure 3b?",
        "III.": "Here are some open-ended questions about the paper:\n\n* What motivated the authors to investigate the ability of large language models (LLMs) to comprehend and generate aligned updates in code-docstring pairs?\n* How do the authors define a correct alignment for RQ1, and what does this imply about the task's requirements?\n* The authors use edit distance as a metric for measuring alignment. Can you explain why they chose this metric, and are there any potential drawbacks or limitations to using it in this context?\n* What is the significance of the 0-shot, 3-shot learning with BM25 retrieval, and 3-shot learning with both BM25 retrieval and contextual information setups in the experiment? How do these setups compare in terms of performance?\n* The results show that Mixtral-8 \u00d722B performs better than Meta Llama-3.1 405B in some cases, particularly in the 3-shot learning setup. Can you speculate why this might be the case, and what could be driving these differences in performance?\n* What insights do the authors' findings provide about the strengths and weaknesses of different language models (LLMs) in tasks related to code and natural language generation?",
        "V.": "What specific changes to a function would be considered tracked by the dataset, and how does it account for cases where the file with the original function is deleted? \n\nHow does the limitation of tracking only changes within the main branch impact the overall utility of the dataset for testing or evaluating changes in codebases that frequently use feature branches or pull requests?",
        "VI.": "What insights or findings from the research presented in this paper were made accessible through the publication of their open-source code?",
        "ACKNOWLEDGMENTS": "What are some potential implications or outcomes of this funding support, as mentioned in the acknowledgment?",
        "REFERENCES": "What is the common theme among the papers cited in this section, particularly those related to software engineering and knowledge engineering? \n\nAre there any notable differences between the types of research presented in papers [2] and [3], which both explore AI-based approaches for code review comments but from different perspectives?\n\nHow do the references in [4] and [5] relate to the broader topic of software development activities, and what insights can be gained from their use of large sequence models? \n\nWhat are some potential applications or implications of the work presented in papers [8], [11], and [12], particularly with regards to mining software repositories and semantic code search?\n\nIn paper [13], a \"guided tour\" is mentioned. What does this imply about the state of knowledge or understanding on approximate string matching at the time of publication, and how has it evolved since then? \n\nHow do papers [14] and [16] contribute to our understanding of algorithms for correcting deletions, insertions, and reversals in strings, as well as the development of new models for semantic code search?\n\nCan you identify any commonalities or similarities between the topics addressed in papers [15], [17], and [18], particularly those related to expert systems and mixed ensemble methods?"
    },
    "Grammar": {
        "I.": {
            "Accept": true,
            "Feedback": "Accept \n\nAccept \n\nAccept \n\nReject ( missing comma after \"code\" in \"function is dropped and docstring-function pair were updated\")\n\nAccept \n\nReject ( incorrect verb tense; \"generate\" should be \"generate new implementation\" for consistency)\n\nAccept \n\nAccept \n\nAccept \n\nAccept \n\nAccept \n\nAccept \n\nAccept \n\nAccept \n\nAccept \n\nAccept \n\nAccept \n\nAccept \n\nReject ( missing comma after \"procedure\")\n\nAccept \n\nAccept \n\nAccept \n\nAccept"
        },
        "II.": {
            "Accept": true,
            "Feedback": "Accept (minor typos only)\nAccept (grammar is mostly correct, but there are a few minor errors)\n\nSpecifically:\n\n* In Figure 2 caption: \"per function\" should be \"per functions\"\n* In Figure 3a caption: \"vsthe\" should be \"vs the\"\n* In Table I description: \"project Name of the project.\" should be \"Project Name of the Project.\"\n* In Table I description: \"file_path Full path to the file\" is not clearly explained, please add a brief explanation.\n* In Figure 5 caption: \"indicating difference in the detail of documentation for functions\" could be rephrased for better clarity.\n* In Figure 4 caption: \"As with a lot of software data, this dataset is skewed, with the top 25 projects accounting for around 60% of the total samples.\" could be rephrased for better clarity.\n\nPlease note that these are minor errors and do not affect the overall understanding of the text."
        },
        "III.": {
            "Accept": true,
            "Feedback": "Accept \n\n(For the entire text)\n\nAccept \n\n(According to your rules, I should only say \"Accept\" if the grammar is correct and \"Reject\" if the grammar is incorrect. Since you didn't specify any specific parts of the text that need checking, I'll assume that the entire text is acceptable.)"
        },
        "V.": {
            "Accept": true,
            "Feedback": "Accept \n\n(Note: The grammar is correct throughout the passage.)"
        },
        "VI.": {
            "Accept": true,
            "Feedback": "Accept"
        },
        "ACKNOWLEDGMENTS": {
            "Accept": false,
            "Feedback": "Reject"
        },
        "REFERENCES": {
            "Accept": true,
            "Feedback": "References\n[1] G. Canfora and A. Cimitile, \u201cSoftware maintenance,\u201d in Handbook of\nSoftware Engineering and Knowledge Engineering: Volume I: Fundamentals . World Scientific, 2001, pp. 91\u2013120.\n[2] C. S. Xia, Y . Wei, and L. Zhang, \u201cAutomated program repair in the\nera of large pre-trained language models,\u201d in 2023 IEEE/ACM 45th\nInternational Conference on Software Engineering (ICSE) . IEEE, 2023,\npp. 1482\u20131494.\n[3] A. Fr\u00f6mmgen, J. Austin, P. Choy, N. Ghelani, L. Kharatyan, G. Surita,\nE. Khrapko, P. Lamblin, P.-A. Manzagol, M. Revaj et al. , \u201cResolving\ncode review comments with machine learning,\u201d in Proceedings of the 46th International Conference on Software Engineering: Software\nEngineering in Practice , 2024, pp. 204\u2013215.\n[4] P. Maniatis and D. Tarlow, \u201cLarge sequence models for software development activities,\u201d https://research.google/blog/\nlarge-sequence-models-for-software-development-activities/.\n[5] Y . Zhang, H. Ruan, Z. Fan, and A. Roychoudhury, \u201cAutocoderover:\nAutonomous program improvement,\u201d in Proceedings of the 33rd ACM\nSIGSOFTInternational Symposium on Software Testing and Analysis ,\n2024, pp. 1592\u20131604.\n[6] M. Visconti and C. R. Cook, \u201cAn overview of industrial software\ndocumentation practice,\u201d in 12th International Conference of the Chilean\nComputer Science Society, 2002. Proceedings. IEEE, 2002, pp. 179\u2013186.\n[7] D. Schreck, V . Dallmeier, and T. Zimmermann, \u201cHow documentation evolves over time,\u201d in Ninth international workshop on Principles of\nsoftware evolution: in conjunction with the 6th ESEC/FSE joint meeting ,\n2007, pp. 4\u201310.\n[8] D. Spadini, M. Aniche, and A. Bacchelli, \u201cPydriller: Python framework for mining software repositories,\u201d in Proceedings of the 2018 26th\nACMJoint meeting on european software engineering conference and\nsymposium on the foundations of software engineering , 2018, pp. 908\u2013911.\n[9] J. Friedl, Mastering regular expressions . \" O\u2019Reilly Media, Inc.\", 2006.\n[10] Tree-sitter, \u201cPython tree-sitter,\u201d https://github.com/tree-sitter/\npy-tree-sitter, 2024.\n[11] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\n\u201cCodesearchnet challenge: Evaluating the state of semantic code search,\u201d\narXiv preprint arXiv:1909.09436 , 2019.\n[12] R. A. Wagner and M. J. Fischer, \u201cThe string-to-string correction\nproblem,\u201d J. ACM , vol. 21, no. 1, p. 168\u2013173, Jan. 1974. [Online].\nAvailable: https://doi.org/10.1145/321796.321811\n[13] G. Navarro, \u201cA guided tour to approximate string matching,\u201d ACM\ncomputing surveys (CSUR) , vol. 33, no. 1, pp. 31\u201388, 2001.\n[14] V . Levenshtein, \u201cBinary codes capable of correcting deletions, insertions,\nand reversals,\u201d Proceedings of the Soviet physics doklady , 1966.\n[15] S. Robertson, H. Zaragoza et al. , \u201cThe probabilistic relevance frame-\nwork: Bm25 and beyond,\u201d Foundations and Trends\u00ae in Information\nRetrieval , vol. 3, no. 4, pp. 333\u2013389, 2009.\n[16] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\nA. Mathur, A. Schelten, A. Yang, A. Fan et al. , \u201cThe llama 3 herd of\nmodels,\u201d arXiv preprint arXiv:2407.21783 , 2024.\n[17] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bam-\nford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand et al. ,\n\u201cMixtral of experts,\u201d arXiv preprint arXiv:2401.04088 , 2024.\n[18] K. Pai, P. Devanbu, and T. Ahmed, \u201cCoDocBench: A dataset for\nCode-Documentation Alignment in Software Maintenance,\u201d Nov. 2024.\n \nAccept"
        }
    },
    "Novelty": {
        "I.": {
            "Feedback": "Accept.\n\nI chose to accept this paper as novel because, although some of its ideas may seem familiar or even presented before (e.g., using language models for software maintenance), the way it tackles specific aspects of code documentation and the proposed dataset CoDocBench seems like a fresh contribution. \n\nThe authors' focus on a particular aspect of software maintenance that has not been extensively explored before, i.e., how code and its associated natural language description (docstrings) are not always kept up to date, suggests that they bring a new perspective or approach to this problem.\n\nMoreover, the proposal of CoDocBench as a specific dataset for training and evaluating language models in tasks related to helping developers better couple code and documentation changes appears to be novel. The emphasis on collecting high-quality samples from GitHub and selecting changes where developers have indeed changed both the code and the documentation in the same commit adds value to the existing body of work.\n\nThe only potential similarity with another paper I found is that it also involves generating docstrings for Python functions using small language models, but whereas the other paper focused on a more general approach to code explanation generation, this one seems to be more specialized and focused on addressing specific issues in software documentation.",
            "Accept": true
        },
        "II.": {
            "Feedback": "I would Reject this paper.\n\nThe reason for this decision is not explicitly stated in the prompt, but based on my analysis, I believe the paper's novelty is not sufficient to justify its originality. The main claims made in the paper are:\n\n1. Developing a dataset (CoDocBench) for code documentation generation.\n2. Proposing a multi-stage fine-tuning strategy and baseline models for code explanation generation tasks.\n3. Investigating the efficacy of small language models (SLMs) for generating high-quality docstrings.\n\nWhile these contributions are significant, they are not entirely novel. The development of datasets for code documentation and explanation generation is not uncommon in the field of natural language processing (NLP). Moreover, the use of fine-tuning strategies and baseline models is a common approach in NLP research.\n\nThe paper's results on SLMs for generating high-quality docstrings are promising, but it would be difficult to determine whether these results are truly novel without more context or comparison to existing work. The fact that the authors mention \"similar papers\" in the arXiv Similar Papers section suggests that there is already some related research in this area.\n\nOverall, while the paper makes some significant contributions to the field of NLP and code documentation generation, its novelty is not sufficient to justify its originality.",
            "Accept": false
        },
        "III.": {
            "Feedback": "I would Reject this paper.\n\nAlthough the paper investigates the ability of large language models (LLMs) to comprehend and generate aligned updates in code-docstring pairs, it does not present any novel contributions or breakthroughs in the field. The research questions and methodology are based on existing work, and the results show that the models struggle with aligning code and docstrings with reference versions.\n\nThe paper also relies heavily on existing papers and datasets, such as CodeExp, DocuMint, and TRAIL, without providing any new insights or perspectives. Furthermore, the proposed approach does not offer any significant improvements over existing methods for generating high-quality docstrings.\n\nAdditionally, the results presented in the paper are limited to a single experiment with two models (Mixtral-8 \u00d722B and Meta Llama-3.1 405B) and only one dataset (2273 test samples). This lack of diversity and scope makes it difficult to generalize the findings to other contexts.\n\nOverall, while the paper may provide some interesting insights into the limitations of current LLMs for code-docstring alignment, it does not offer any novel or groundbreaking contributions that would justify its inclusion in a top-tier conference or journal.",
            "Accept": false
        },
        "V.": {
            "Feedback": "Accept.\n\nAlthough the paper discusses limitations, such as the inability to track changes to a function if the file name remains consistent or is relocated to another file, it presents a novel approach called TRAIL for maintaining traceability information in a system. The novelty of TRAIL lies in its use of previously captured knowledge about project traceability to train a machine learning classifier that can derive new traceability links and update existing ones.\n\nIn contrast, the limitations mentioned earlier are common constraints found in many other research papers on code analysis and maintenance, such as tracking changes over time or maintaining accuracy across different file names. The paper's focus on addressing the challenges of software traceability and proposing a novel solution makes it stand out from similar work.\n\nAdditionally, the paper's results demonstrate that TRAIL outperforms seven popular information retrieval techniques in terms of precision, recall, and F-score, which is a significant contribution to the field of software engineering. Therefore, despite some limitations, the paper presents a novel and promising approach to maintaining traceability information in software systems, making it worthy of consideration for acceptance.",
            "Accept": true
        },
        "VI.": {
            "Feedback": "Accept.\n\nI chose \"Accept\" because, despite some minor criticisms, the paper appears to be novel and contributes significantly to the field of software engineering. Here's why:\n\n1. **New approach**: The paper proposes a novel approach called TRAIL for maintaining traceability information in a system, which is a significant contribution to the field.\n2. **Machine learning-based solution**: The use of machine learning to derive new traceability links and update existing ones is a novel application of this technology.\n3. **Comparison with existing approaches**: The paper compares TRAIL with seven popular information retrieval techniques, including some commonly used in previous work, which shows that the proposed approach outperforms them in terms of precision, recall, and F-score.\n4. **Case studies and validation**: The authors report on case studies and validate their results using 11 commonly used traceability datasets from six software systems, which adds credibility to their claims.\n5. **Contribution to software engineering**: The paper addresses a significant problem in software engineering, namely the maintenance of traceability links during project evolution, and provides a novel solution that can improve project outcomes.\n\nWhile some minor criticisms could be raised, such as the lack of comprehensive evaluation or comparison with other machine learning-based approaches, these do not seem significant enough to outweigh the overall novelty and contribution of the paper.",
            "Accept": true
        },
        "ACKNOWLEDGMENTS": {
            "Feedback": "I would reject this paper.\n\nAlthough the paper presents some novel ideas, such as proposing a code explanation generation task and developing a framework for maintaining traceability information in a system (TRAIL), it does not necessarily contribute to the field in a significantly new or innovative way. The authors rely heavily on existing work, including their own previous research, and do not provide enough new insights or contributions to justify its publication.\n\nThe paper appears to be an extension of existing research rather than a truly novel contribution. For example, the authors mention that they have already conducted a human study to identify criteria for high-quality explanatory docstrings, but they do not discuss any new findings or methodologies that are significantly different from their previous work.\n\nFurthermore, the paper's focus on code explanation generation and traceability maintenance is an area of research that has been explored by other researchers in recent years. The authors' proposal of a novel approach (TRAIL) does not seem to offer anything substantially new or improved over existing solutions.\n\nIn addition, the paper's presentation and writing style do not suggest any significant advances in methodology or innovative problem-solving. The text is mostly descriptive and lacks clarity on how the proposed approaches overcome existing challenges or limitations in the field.\n\nOverall, while the paper may have some interesting ideas, it does not seem to offer enough new insights or contributions to justify its publication in a top-tier conference or journal.",
            "Accept": false
        },
        "REFERENCES": {
            "Feedback": "I would say \"Accept\".\n\nThe paper presents a novel approach for maintaining traceability information in a software system, which is an important aspect of software maintenance and evolution. The authors propose a machine learning-based approach called TRAIL, which leverages previously captured knowledge about project traceability to train a classifier that can derive new traceability links and update existing ones.\n\nThe paper addresses a significant problem in the field of software engineering, which is the maintenance of outdated traceability information during software maintenance and evolution. The proposed approach is novel and innovative, as it uses machine learning techniques to improve the accuracy of traceability link derivation and update.\n\nWhile the paper builds upon previous work in the area of software traceability, its contribution is significant and novel. The authors' use of a large-scale dataset and their evaluation of TRAIL against seven popular information retrieval techniques demonstrate the effectiveness of their approach.\n\nOverall, the paper presents a well-researched and well-structured proposal for a novel approach to maintaining traceability information in software systems. Therefore, I would accept the paper as novel and significant contribution to the field of software engineering.",
            "Accept": true
        }
    },
    "FactChecker": {
        "I.": {
            "Feedback": "[Reject]\n\nThe claims made in the text contain several inaccuracies and oversimplifications. Specifically:\n\n1. The claim that 60-80% of overall software budgets are consumed by maintenance activities is likely an exaggeration, as stated by the IEEE.\n2. The assumption that all LLM applications to software maintenance have been widely adopted and successful is not supported by evidence.\n3. The statement that documentation practice in industry needs improvement lacks concrete evidence to support it.\n4. The reference [7] to Schrek et al. is missing, which makes it difficult to verify the accuracy of this claim.\n\nI would suggest rephrasing these claims to reflect a more nuanced understanding of software maintenance and documentation practices.\n\nAdditionally, I would recommend providing more context or evidence to support the figure that \"Docstrings are updated only around 33% of the time that code is changed\". This seems like an unlikely statistic without further justification.",
            "Accept": false
        },
        "II.": {
            "Feedback": "[Reject]\n\nHere are some specific corrections and clarifications:\n\n1. PyDriller is indeed a Python framework for mining Git, but its ability to process large amounts of Git data efficiently is not explicitly stated in the provided text.\n2. The statement about Tree-Sitter seems unclear because it does not provide enough context on how the library is being used in this specific step. I couldn't find any direct reference or explanation of how it's fixing associations between function names and related data.\n\n3. The terms \"commit messages\", \"code diff\", and \"docstring diff\" are not clearly defined in the provided text, making it unclear what they represent or how they were calculated.\n\n4. Figure 2 is indeed a workflow diagram for creating CoDocBench, but without more context, I couldn't clarify each stage's representation or data collection process.\n\nAdditionally, my take on these facts would be that while the text appears to describe the creation of the CoDocBench dataset in detail, there are unclear statements and missing references that need clarification.",
            "Accept": false
        },
        "III.": {
            "Feedback": "[Reject]\n\nThe article contains several inaccuracies and unclear statements.\n\n*   There is no clear explanation of how edit distance [12]\u2013[14] was defined or calculated.\n*   The definition of correct alignment for RQ1 is not clearly stated, but it seems to imply perfect temporal alignment between the old and new code/docstring pairs. However, the article does not provide a detailed explanation of what this means in practice.\n*   The performance metrics used for RQ2 are unclear. The article mentions that lower edit distance suggests that the model's generation saves developers some editing work, but it is not clear how this metric is calculated or what its significance is.\n\nThe article also makes claims about the performance of Mixtral-8 \u00d722B and Meta Llama-3.1 405B, but these are based on incomplete or unclear information. The statement that Mixtral-8 \u00d722B largely performs better with the 3-shot learning setup compared to Meta Llama-3.1 405B is not supported by any clear evidence or explanation.\n\nFurthermore, the article does not provide enough context about the datasets used in the experiments, the evaluation metrics, or the baseline models. This makes it difficult to evaluate the results and draw conclusions about the effectiveness of the proposed methods.\n\nOverall, I would recommend re-examining the research questions and methods to ensure that they are clearly defined and accurately implemented.",
            "Accept": false
        },
        "V.": {
            "Feedback": "[Reject]\n\nSpecific corrections:\n\n* The text does not provide a reference for these claims.\n* The limitations mentioned do not seem to be unique to the dataset itself but rather general constraints on tracking changes to functions and files in software development.\n* The rationale provided seems overly simplistic and may not fully capture the complexities of software maintenance and version control.\n\nMy take: While it's understandable that the dataset might prioritize consistency and stability, this limitation could potentially lead to incomplete or outdated information about code changes over time. A more nuanced approach would consider the trade-offs between consistency, completeness, and timeliness in tracking code changes.",
            "Accept": false
        },
        "VI.": {
            "Feedback": "Reject\n\nThe provided text contains several inaccuracies:\n\n- The statement \"Zenodo was launched on 8 May 2013\" is not accurate. According to the Wikipedia article, Zenodo was relaunched as Zenodo in 2015.\n\n- While it is true that Zenodo provides a persistent digital object identifier (DOI) for each submission, there is no mention of GitHub repositories being supported by Zenodo itself.\n\n- The provided link https://doi.org/10.5281/zenodo.14251622 appears to be incorrect or incomplete as the actual DOI for a specific item on Zenodo would likely include more information and context.",
            "Accept": false
        },
        "ACKNOWLEDGMENTS": {
            "Feedback": "[Reject]\n\nThe provided statement appears to lack crucial details such as the grant number, principal investigator's name, and a brief description of the research. A more accurate acknowledgement should include these essential components to provide transparency and accountability regarding the funding source.\n\nHere is an example of a corrected acknowledgement:\n\n\"This work was partially supported by the National Science Foundation under Grant Number CISE SHF MEDIUM 2107592, Principal Investigator [Name], for the project titled '[Project Title]', which aims to [briefly describe the research objectives].\"\n\nBy including these essential details, the acknowledgment becomes more informative and helpful in understanding the scope of the research.",
            "Accept": false
        },
        "REFERENCES": {
            "Feedback": "[Accept]\n\nThe references appear to be a collection of publications related to software engineering, artificial intelligence, and natural language processing. The sources range from academic papers to conference proceedings and books.\n\nUpon reviewing the list, I found no major inaccuracies or errors in the citations or references provided. However, it's worth noting that some sources may not have been peer-reviewed or published in reputable journals, which could impact their credibility.",
            "Accept": true
        }
    }
}